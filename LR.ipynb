{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from scipy import spatial\n",
    "import csv\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = './Quora_question_pair_partition/'\n",
    "train_dir = base_dir + 'train.tsv'\n",
    "dev_dir = base_dir + 'dev.tsv'\n",
    "test_dir = base_dir + 'test.tsv'\n",
    "word_dir = base_dir + 'wordvec.txt'\n",
    "words_vec = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n",
      "30000\n",
      "35000\n",
      "40000\n",
      "45000\n",
      "50000\n",
      "55000\n",
      "60000\n",
      "65000\n",
      "70000\n",
      "75000\n",
      "80000\n",
      "85000\n",
      "90000\n",
      "95000\n",
      "100000\n",
      "105000\n"
     ]
    }
   ],
   "source": [
    "def loadWordDic():\n",
    "    with open(word_dir, 'r') as words:\n",
    "        tweets_reader = csv.reader(words, delimiter=' ')\n",
    "        cnt = 0\n",
    "        for row in tweets_reader:\n",
    "            words_vec[row[0]] = np.array([float(x) for x in row[1:]]).reshape((1,300))\n",
    "            cnt += 1\n",
    "            if cnt%5000 == 0:\n",
    "                print(cnt)\n",
    "loadWordDic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.5415105483333338\n"
     ]
    }
   ],
   "source": [
    "words1 = ['What', 'is', 'your', 'review', 'of', 'Hidden', 'Figures', '-LRB-', '2016', 'movie', '-RRB-', '?']\n",
    "words2 = ['What', 'are', 'your', 'impressions', 'of', 'Hidden', 'Figures', '-LRB-', '2017', 'movie', '-RRB-', '?']\n",
    "v1 = np.zeros((1,300))\n",
    "cnt1 = 0\n",
    "v2 = np.zeros((1,300))\n",
    "cnt2 = 0\n",
    "for word in words1:\n",
    "    if word.lower() in words_vec:\n",
    "        cnt1 += 1\n",
    "        v1 += words_vec[word.lower()]\n",
    "\n",
    "for word in words2:\n",
    "    if word.lower() in words_vec:\n",
    "        cnt2 += 1\n",
    "        v2 += words_vec[word.lower()]\n",
    "\n",
    "print ((v1 / cnt1 - v2  / cnt2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n",
      "0.75\n"
     ]
    }
   ],
   "source": [
    "words1 = ['What', 'is', 'your', 'review', 'of', 'Hidden', 'Figures', '-LRB-', '2016', 'movie', '-RRB-', '?']\n",
    "words2 = ['What', 'are', 'your', 'impressions', 'of', 'Hidden', 'Figures', '-LRB-', '2017', 'movie', '-RRB-', '?']\n",
    "\n",
    "same_count = 0\n",
    "for word in words1:\n",
    "    if word in words2:\n",
    "        same_count += 1\n",
    "\n",
    "print(same_count / len(words1))\n",
    "print(same_count / len(words2))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.5817388122944436, 1.0, 0.35610330124787437, 1.0, 1.0, 1.0, 1.0, 0.8642884698482242, 1.0, 1.0, 1.0]\n",
      "0.9001775486158786\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "words1 = ['What', 'is', 'your', 'review', 'of', 'Hidden', 'Figures', '-LRB-', '2016', 'movie', '-RRB-', '?']\n",
    "words2 = ['What', 'are', 'your', 'impressions', 'of', 'Hidden', 'Figures', '-LRB-', '2017', 'movie', '-RRB-', '?']\n",
    "\n",
    "max_similarity = []\n",
    "\n",
    "for word1 in words1:\n",
    "    temp = 0\n",
    "    for word2 in words2:\n",
    "        if(word1.lower() in words_vec and word2.lower() in words_vec):\n",
    "            value = 1 - spatial.distance.cosine(words_vec[word1.lower()], words_vec[word2.lower()])\n",
    "            #print(word1 + ' ' + word2 + ' ' + str(value))\n",
    "            if(value > temp):\n",
    "                temp = value\n",
    "    max_similarity.append(temp)\n",
    "\n",
    "print(max_similarity)\n",
    "print(np.mean(max_similarity))\n",
    "print(np.median(max_similarity))\n",
    "max_similarity.sort(reverse=True)\n",
    "print(np.mean(max_similarity[:(len(max_similarity)//2)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "def loadData():\n",
    "    train_df = pd.read_csv(train_dir,sep='\\t', header=None, names=['label','q1','q2','id'],nrows =10000)\n",
    "    train_X, train_y = train_df[['q1','q2']], train_df['label']\n",
    "    dev_df = pd.read_csv(dev_dir,sep='\\t', header=None, names=['label','q1','q2','id'])\n",
    "    dev_X, dev_y = dev_df[['q1','q2']], dev_df['label']\n",
    "    test_df = pd.read_csv(test_dir,sep='\\t', header=None, names=['label','q1','q2','id'])\n",
    "    test_X, test_y = test_df[['q1','q2']], test_df['label']\n",
    "    return train_X, train_y, dev_X, dev_y, test_X, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_X, train_y, dev_X, dev_y, test_X, test_y = loadData()\n",
    "#temp = test_X.apply(lambda x: [1, 2], axis=1)\n",
    "#temp.columns = [\"new1\",\"new2\"]\n",
    "#print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fre1(row):\n",
    "    words1 = row['q1'].split(' ')\n",
    "    words2 = row['q2'].split(' ')\n",
    "    same_count = 0\n",
    "    for word in words1:\n",
    "        if word in words2:\n",
    "            same_count += 1\n",
    "\n",
    "    return (same_count / len(words1))\n",
    "\n",
    "def fre2(row):\n",
    "    words1 = row['q1'].split(' ')\n",
    "    words2 = row['q2'].split(' ')\n",
    "    same_count = 0\n",
    "    for word in words1:\n",
    "        if word in words2:\n",
    "            same_count += 1\n",
    "\n",
    "    return (same_count / len(words2))\n",
    "\n",
    "def dis_mean1(row):\n",
    "    words1 = row['q1'].split(' ')\n",
    "    words2 = row['q2'].split(' ')\n",
    "\n",
    "    max_similarity = []\n",
    "    for word1 in words1:\n",
    "        temp = 0\n",
    "        for word2 in words2:\n",
    "            if (word1.lower() in words_vec and word2.lower() in words_vec):\n",
    "                value = 1 - spatial.distance.cosine(words_vec[word1.lower()], words_vec[word2.lower()])\n",
    "                # print(word1 + ' ' + word2 + ' ' + str(value))\n",
    "                if (value > temp):\n",
    "                    temp = value\n",
    "        max_similarity.append(temp)\n",
    "\n",
    "    value = np.mean(max_similarity)\n",
    "    if(math.isnan(value)):\n",
    "        print(\"nan occur\")\n",
    "        return 0.5\n",
    "\n",
    "    return value\n",
    "\n",
    "\n",
    "def dis_median1(row):\n",
    "    words1 = row['q1'].split(' ')\n",
    "    words2 = row['q2'].split(' ')\n",
    "\n",
    "    max_similarity = []\n",
    "    for word1 in words1:\n",
    "        temp = 0\n",
    "        for word2 in words2:\n",
    "            if (word1.lower() in words_vec and word2.lower() in words_vec):\n",
    "                value = 1 - spatial.distance.cosine(words_vec[word1.lower()], words_vec[word2.lower()])\n",
    "                # print(word1 + ' ' + word2 + ' ' + str(value))\n",
    "                if (value > temp):\n",
    "                    temp = value\n",
    "        max_similarity.append(temp)\n",
    "\n",
    "    value = np.median(max_similarity)\n",
    "    if(math.isnan(value)):\n",
    "        print(\"nan occur\")\n",
    "        return 0.5\n",
    "\n",
    "    return value\n",
    "\n",
    "def dis_core_mean1(row):\n",
    "    words1 = row['q1'].split(' ')\n",
    "    words2 = row['q2'].split(' ')\n",
    "\n",
    "    max_similarity = []\n",
    "    for word1 in words1:\n",
    "        temp = 0\n",
    "        for word2 in words2:\n",
    "            if (word1.lower() in words_vec and word2.lower() in words_vec):\n",
    "                value = 1 - spatial.distance.cosine(words_vec[word1.lower()], words_vec[word2.lower()])\n",
    "                # print(word1 + ' ' + word2 + ' ' + str(value))\n",
    "                if (value > temp):\n",
    "                    temp = value\n",
    "        max_similarity.append(temp)\n",
    "\n",
    "    max_similarity.sort(reverse=True)\n",
    "    value = np.mean(max_similarity[:(len(max_similarity) // 2)])\n",
    "\n",
    "    if(math.isnan(value)):\n",
    "        print(\"nan occur\")\n",
    "        return 0.5\n",
    "\n",
    "    return value\n",
    "\n",
    "def dis_mean2(row):\n",
    "    words1 = row['q2'].split(' ')\n",
    "    words2 = row['q1'].split(' ')\n",
    "\n",
    "    max_similarity = []\n",
    "    for word1 in words1:\n",
    "        temp = 0\n",
    "        for word2 in words2:\n",
    "            if (word1.lower() in words_vec and word2.lower() in words_vec):\n",
    "                value = 1 - spatial.distance.cosine(words_vec[word1.lower()], words_vec[word2.lower()])\n",
    "                # print(word1 + ' ' + word2 + ' ' + str(value))\n",
    "                if (value > temp):\n",
    "                    temp = value\n",
    "        max_similarity.append(temp)\n",
    "\n",
    "    value = np.mean(max_similarity)\n",
    "    if(math.isnan(value)):\n",
    "        print(\"nan occur\")\n",
    "        return 0.5\n",
    "\n",
    "    return value\n",
    "\n",
    "\n",
    "def dis_median2(row):\n",
    "    words1 = row['q2'].split(' ')\n",
    "    words2 = row['q1'].split(' ')\n",
    "\n",
    "    max_similarity = []\n",
    "    for word1 in words1:\n",
    "        temp = 0\n",
    "        for word2 in words2:\n",
    "            if (word1.lower() in words_vec and word2.lower() in words_vec):\n",
    "                value = 1 - spatial.distance.cosine(words_vec[word1.lower()], words_vec[word2.lower()])\n",
    "                # print(word1 + ' ' + word2 + ' ' + str(value))\n",
    "                if (value > temp):\n",
    "                    temp = value\n",
    "        max_similarity.append(temp)\n",
    "\n",
    "    value = np.median(max_similarity)\n",
    "    if(math.isnan(value)):\n",
    "        print(\"nan occur\")\n",
    "        return 0.5\n",
    "\n",
    "    return value\n",
    "\n",
    "def dis_core_mean2(row):\n",
    "    words1 = row['q2'].split(' ')\n",
    "    words2 = row['q1'].split(' ')\n",
    "\n",
    "    max_similarity = []\n",
    "    for word1 in words1:\n",
    "        temp = 0\n",
    "        for word2 in words2:\n",
    "            if (word1.lower() in words_vec and word2.lower() in words_vec):\n",
    "                value = 1 - spatial.distance.cosine(words_vec[word1.lower()], words_vec[word2.lower()])\n",
    "                # print(word1 + ' ' + word2 + ' ' + str(value))\n",
    "                if (value > temp):\n",
    "                    temp = value\n",
    "        max_similarity.append(temp)\n",
    "\n",
    "    max_similarity.sort(reverse=True)\n",
    "    value = np.mean(max_similarity[:(len(max_similarity) // 2)])\n",
    "\n",
    "    if (math.isnan(value)):\n",
    "        print(\"nan occur\")\n",
    "        return 0.5\n",
    "\n",
    "    return value\n",
    "\n",
    "def rate_len(row):\n",
    "    words1 = row['q1'].split(' ')\n",
    "    words2 = row['q2'].split(' ')\n",
    "    len1 = len(words1)\n",
    "    len2 = len(words2)\n",
    "\n",
    "    rate = len1/len2\n",
    "    if(rate > 1):\n",
    "        rate = 1/rate\n",
    "\n",
    "    return rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(df):\n",
    "    feature_names = []\n",
    "\n",
    "    # number of words\n",
    "    df['cnt'] = df.apply(lambda row: abs(len(row['q1'].split(' ')) - len(row['q2'].split(' '))), axis=1)\n",
    "    feature_names.append('cnt')\n",
    "\n",
    "    # distance between sentences\n",
    "    #df['dis'] = df.apply(dis, axis=1)\n",
    "    #feature_names.append('dis')\n",
    "\n",
    "    # magic feature: question frequency\n",
    "\n",
    "    # same word frequency\n",
    "    df['fre1'] = df.apply(fre1, axis=1)\n",
    "    feature_names.append('fre1')\n",
    "    df['fre2'] = df.apply(fre2, axis=1)\n",
    "    feature_names.append('fre2')\n",
    "\n",
    "    ''' try to add more than one columns one time but failed\n",
    "    temp = df.apply(fre3, axis=1)\n",
    "    temp.columns = ['fre1', 'fre2']\n",
    "    df.join(temp.to_frame())\n",
    "    df[['q1','q2']] = df.apply(fre3, axis=1)\n",
    "    feature_names.append('q1')\n",
    "    feature_names.append('q2')\n",
    "    '''\n",
    "\n",
    "    # cosine based distance statistics\n",
    "    df['dis_mean1'] = df.apply(dis_mean1, axis=1)\n",
    "    feature_names.append('dis_mean1')\n",
    "    #df['dis_core_mean1'] = df.apply(dis_core_mean1, axis=1)\n",
    "    #feature_names.append('dis_core_mean1')\n",
    "    df['dis_median1'] = df.apply(dis_median1, axis=1)\n",
    "    feature_names.append('dis_median1')\n",
    "    df['dis_mean2'] = df.apply(dis_mean2, axis=1)\n",
    "    feature_names.append('dis_mean2')\n",
    "    #df['dis_core_mean2'] = df.apply(dis_core_mean2, axis=1)\n",
    "    #feature_names.append('dis_core_mean2')\n",
    "    df['dis_median2'] = df.apply(dis_median2, axis=1)\n",
    "    feature_names.append('dis_median2')\n",
    "\n",
    "    # relative length rate\n",
    "    df['rate_len'] = df.apply(rate_len, axis=1)\n",
    "    feature_names.append('rate_len')\n",
    "\n",
    "    print(\"Finish generating features\")\n",
    "\n",
    "    return feature_names, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish generating features\n"
     ]
    }
   ],
   "source": [
    "feature_names2, df2 = generate_features(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish loading data\n",
      "Finish generating features\n"
     ]
    }
   ],
   "source": [
    "train_X, train_y, dev_X, dev_y, test_X, test_y = loadData()\n",
    "print(\"Finish loading data\")\n",
    "feature_names, df = generate_features(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the training set: 0.6998\n",
      "Accuracy on the testing set: 0.7015\n"
     ]
    }
   ],
   "source": [
    "logreg = linear_model.LogisticRegression(penalty='l2',C=1e7)\n",
    "\n",
    "logreg.fit(df[feature_names], train_y)\n",
    "\n",
    "acc = logreg.score(df[feature_names], train_y)\n",
    "\n",
    "print(\"Accuracy on the training set:\", acc)\n",
    "\n",
    "acc = logreg.score(df2[feature_names2], test_y)\n",
    "\n",
    "print(\"Accuracy on the testing set:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
